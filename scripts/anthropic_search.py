#!/usr/bin/env python3
"""
Anthropic Claude Web Search & Fetch ìœ í‹¸ë¦¬í‹°
Claude Messages APIì˜ web_search + web_fetch ì„œë²„ ë„êµ¬ë¥¼ í™œìš©í•œ ê²€ìƒ‰ ìŠ¤í¬ë¦½íŠ¸.

ì°¸ê³  ë¬¸ì„œ:
- Web Search: https://platform.claude.com/docs/ko/agents-and-tools/tool-use/web-search-tool
- Web Fetch: https://platform.claude.com/docs/ko/agents-and-tools/tool-use/web-fetch-tool

ì‚¬ìš©ë²•:
    python3 scripts/anthropic_search.py "ê²€ìƒ‰ì–´"
    python3 scripts/anthropic_search.py "ê²€ìƒ‰ì–´" --mode search|verify|deep
    python3 scripts/anthropic_search.py "ê²€ìƒ‰ì–´" --fetch  # ê²€ìƒ‰ í›„ ìƒìœ„ ê²°ê³¼ í˜ì¹˜
    python3 scripts/anthropic_search.py "ê²€ìƒ‰ì–´" --dynamic  # ë™ì  í•„í„°ë§ (Opus 4.6/Sonnet 4.6)
"""

import argparse
import json
import os
import sys
from datetime import datetime


def get_api_key():
    key = os.environ.get("ANTHROPIC_API_KEY")
    if not key:
        print("ERROR: ANTHROPIC_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
        sys.exit(1)
    return key


def search(
    query: str,
    mode: str = "search",
    lang: str = "both",
    model: str = "claude-sonnet-4-6",
    max_search_uses: int = 5,
    allowed_domains: list[str] | None = None,
    blocked_domains: list[str] | None = None,
    enable_fetch: bool = False,
    dynamic_filtering: bool = False,
    user_location: dict | None = None,
) -> dict:
    """
    Claude Messages API + web_search / web_fetch ì„œë²„ ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰.

    ë„êµ¬ íƒ€ì…:
    - web_search_20250305: ê¸°ë³¸ ì›¹ ê²€ìƒ‰
    - web_search_20260209: ë™ì  í•„í„°ë§ ì§€ì› (Opus 4.6, Sonnet 4.6)
    - web_fetch_20250910: ê¸°ë³¸ ì›¹ í˜ì¹˜
    - web_fetch_20260209: ë™ì  í•„í„°ë§ ì§€ì›

    ë™ì  í•„í„°ë§ì€ code-execution-web-tools-2026-02-09 ë² íƒ€ í—¤ë” í•„ìš”.
    """
    import urllib.request
    import urllib.error

    api_key = get_api_key()

    # ëª¨ë“œë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompts = {
        "search": (
            "ë‹¹ì‹ ì€ ì›¹ ë¦¬ì„œì¹˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ í™œìš©í•˜ì—¬ ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ "
            "í¬ê´„ì ì´ê³  ì •í™•í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”. ëª¨ë“  ì •ë³´ì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”. "
            "í•œêµ­ì–´ì™€ ì˜ì–´ ì†ŒìŠ¤ë¥¼ ëª¨ë‘ í™œìš©í•˜ì„¸ìš”."
        ),
        "verify": (
            "ë‹¹ì‹ ì€ íŒ©íŠ¸ì²´í¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì£¼ì–´ì§„ ì£¼ì¥ì˜ ì •í™•ì„±ì„ ê²€ì¦í•˜ì„¸ìš”. "
            "ì›ë³¸ ì¶œì²˜ë¥¼ ì¶”ì í•˜ê³ , ê²€ì¦ ê²°ê³¼ë¥¼ 'í™•ì¸ë¨/ë¶€ë¶„í™•ì¸/ë¯¸í™•ì¸/ì˜¤ë¥˜'ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”."
        ),
        "deep": (
            "ë‹¹ì‹ ì€ ì‹¬ì¸µ ë¦¬ì„œì¹˜ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ì£¼ì œì˜ "
            "ë‹¤ì–‘í•œ ì¸¡ë©´(ì—­ì‚¬, í˜„ì¬, ë¯¸ë˜ ì „ë§, ì°¬ë°˜ ì˜ê²¬)ì„ ëª¨ë‘ ì¡°ì‚¬í•˜ì„¸ìš”. "
            "í•™ìˆ  ìë£Œ, ì—…ê³„ ë³´ê³ ì„œ, ë‰´ìŠ¤ ê¸°ì‚¬ ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ë¥¼ í™œìš©í•˜ì„¸ìš”."
        ),
    }

    system_prompt = system_prompts.get(mode, system_prompts["search"])

    # ì–¸ì–´ë³„ ê²€ìƒ‰ì–´ ì¡°ì •
    if lang == "ko":
        user_query = f"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ í•œêµ­ì–´ ì†ŒìŠ¤ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê²€ìƒ‰í•´ ì£¼ì„¸ìš”: {query}"
    elif lang == "en":
        user_query = f"Search comprehensively for the following topic in English: {query}"
    else:
        user_query = f"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ í•œêµ­ì–´ì™€ ì˜ì–´ ì†ŒìŠ¤ë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ í¬ê´„ì ìœ¼ë¡œ ê²€ìƒ‰í•´ ì£¼ì„¸ìš”: {query}"

    # ë„êµ¬ ì„¤ì •
    if dynamic_filtering:
        search_tool_type = "web_search_20260209"
        fetch_tool_type = "web_fetch_20260209"
    else:
        search_tool_type = "web_search_20250305"
        fetch_tool_type = "web_fetch_20250910"

    web_search_tool = {
        "type": search_tool_type,
        "name": "web_search",
        "max_uses": max_search_uses,
    }

    if allowed_domains:
        web_search_tool["allowed_domains"] = allowed_domains
    if blocked_domains:
        web_search_tool["blocked_domains"] = blocked_domains
    if user_location:
        web_search_tool["user_location"] = user_location

    tools = [web_search_tool]

    if enable_fetch:
        fetch_tool = {
            "type": fetch_tool_type,
            "name": "web_fetch",
            "max_uses": 5,
            "citations": {"enabled": True},
        }
        if allowed_domains:
            fetch_tool["allowed_domains"] = allowed_domains
        tools.append(fetch_tool)

    payload = {
        "model": model,
        "max_tokens": 4096,
        "system": system_prompt,
        "messages": [
            {"role": "user", "content": user_query},
        ],
        "tools": tools,
    }

    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01",
    }

    if dynamic_filtering:
        headers["anthropic-beta"] = "code-execution-web-tools-2026-02-09"

    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        "https://api.anthropic.com/v1/messages",
        data=data,
        headers=headers,
        method="POST",
    )

    try:
        with urllib.request.urlopen(req, timeout=180) as resp:
            result = json.loads(resp.read().decode("utf-8"))
    except urllib.error.HTTPError as e:
        error_body = e.read().decode("utf-8") if e.fp else ""
        print(f"ERROR: Anthropic API í˜¸ì¶œ ì‹¤íŒ¨ (HTTP {e.code}): {error_body}", file=sys.stderr)
        sys.exit(1)
    except urllib.error.URLError as e:
        print(f"ERROR: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e.reason}", file=sys.stderr)
        sys.exit(1)

    return result


def extract_response(result: dict) -> str:
    """
    Claude Messages API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ì™€ ì¸ìš© ì¶”ì¶œ.

    ì‘ë‹µ content ë°°ì—´ êµ¬ì¡°:
    - type: "text" â†’ í…ìŠ¤íŠ¸ (citations ë°°ì—´ í¬í•¨ ê°€ëŠ¥)
      - citation.type: "web_search_result_location" â†’ url, title, cited_text
    - type: "server_tool_use" â†’ ê²€ìƒ‰/í˜ì¹˜ ì‹¤í–‰ (name: "web_search" | "web_fetch")
    - type: "web_search_tool_result" â†’ ê²€ìƒ‰ ê²°ê³¼
      - content[].type: "web_search_result" â†’ url, title, page_age, encrypted_content
    - type: "web_fetch_tool_result" â†’ í˜ì¹˜ ê²°ê³¼
    """
    output_parts = []
    citations = []
    search_results = []

    content_blocks = result.get("content", [])

    for block in content_blocks:
        block_type = block.get("type")

        # í…ìŠ¤íŠ¸ ë¸”ë¡ (ì¸ìš© í¬í•¨ ê°€ëŠ¥)
        if block_type == "text":
            text = block.get("text", "")
            if text.strip():
                output_parts.append(text)

            # ì¸ìš© ì¶”ì¶œ
            for citation in block.get("citations", []):
                if citation.get("type") == "web_search_result_location":
                    citations.append({
                        "url": citation.get("url", ""),
                        "title": citation.get("title", ""),
                        "cited_text": citation.get("cited_text", ""),
                    })
                elif citation.get("type") == "char_location":
                    citations.append({
                        "url": "",
                        "title": citation.get("document_title", ""),
                        "cited_text": citation.get("cited_text", ""),
                    })

        # ê²€ìƒ‰ ê²°ê³¼
        elif block_type == "web_search_tool_result":
            for item in block.get("content", []):
                if isinstance(item, dict) and item.get("type") == "web_search_result":
                    search_results.append({
                        "url": item.get("url", ""),
                        "title": item.get("title", ""),
                        "page_age": item.get("page_age", ""),
                    })

    text = "\n".join(output_parts)

    # ì¸ìš©
    if citations:
        text += "\n\n---\n### ì¸ìš© (Citations)\n"
        seen = set()
        for c in citations:
            key = c["url"] or c["title"]
            if key and key not in seen:
                seen.add(key)
                if c["url"]:
                    text += f"- [{c['title']}]({c['url']})\n"
                else:
                    text += f"- {c['title']}\n"
                if c["cited_text"]:
                    text += f"  > {c['cited_text'][:150]}...\n"

    # ê²€ìƒ‰ì—ì„œ ë°œê²¬ëœ ì†ŒìŠ¤
    if search_results:
        text += "\n### ê²€ìƒ‰ ê²°ê³¼ ì†ŒìŠ¤\n"
        seen_urls = set()
        for sr in search_results:
            if sr["url"] and sr["url"] not in seen_urls:
                seen_urls.add(sr["url"])
                age_str = f" ({sr['page_age']})" if sr["page_age"] else ""
                text += f"- [{sr['title']}]({sr['url']}){age_str}\n"

    # ì‚¬ìš©ëŸ‰ ì •ë³´
    usage = result.get("usage", {})
    server_tool_use = usage.get("server_tool_use", {})
    search_count = server_tool_use.get("web_search_requests", 0)
    fetch_count = server_tool_use.get("web_fetch_requests", 0)
    if search_count or fetch_count:
        text += f"\n_ì‚¬ìš©ëŸ‰: ê²€ìƒ‰ {search_count}íšŒ, í˜ì¹˜ {fetch_count}íšŒ_\n"

    return text


def main():
    parser = argparse.ArgumentParser(description="Anthropic Claude Web Search ìœ í‹¸ë¦¬í‹°")
    parser.add_argument("query", help="ê²€ìƒ‰í•  ì£¼ì œ ë˜ëŠ” ì§ˆë¬¸")
    parser.add_argument(
        "--mode",
        choices=["search", "verify", "deep"],
        default="search",
        help="ê²€ìƒ‰ ëª¨ë“œ: search(ì¼ë°˜), verify(íŒ©íŠ¸ì²´í¬), deep(ì‹¬ì¸µ)",
    )
    parser.add_argument(
        "--lang",
        choices=["ko", "en", "both"],
        default="both",
        help="ê²€ìƒ‰ ì–¸ì–´: ko(í•œêµ­ì–´), en(ì˜ì–´), both(ì–‘ìª½)",
    )
    parser.add_argument(
        "--model",
        default="claude-sonnet-4-6",
        help="ì‚¬ìš©í•  Claude ëª¨ë¸ (ê¸°ë³¸: claude-sonnet-4-6)",
    )
    parser.add_argument(
        "--max-searches",
        type=int,
        default=5,
        help="ìµœëŒ€ ê²€ìƒ‰ íšŸìˆ˜ (ê¸°ë³¸: 5)",
    )
    parser.add_argument(
        "--domains",
        default=None,
        help="í—ˆìš© ë„ë©”ì¸ (ì½¤ë§ˆ êµ¬ë¶„)",
    )
    parser.add_argument(
        "--block-domains",
        default=None,
        help="ì°¨ë‹¨ ë„ë©”ì¸ (ì½¤ë§ˆ êµ¬ë¶„)",
    )
    parser.add_argument(
        "--fetch",
        action="store_true",
        help="web_fetchë„ í•¨ê»˜ í™œì„±í™” (ê²€ìƒ‰ í›„ ìƒìœ„ ê²°ê³¼ í˜ì¹˜)",
    )
    parser.add_argument(
        "--dynamic",
        action="store_true",
        help="ë™ì  í•„í„°ë§ í™œì„±í™” (Opus 4.6/Sonnet 4.6 ì „ìš©)",
    )
    parser.add_argument("--raw", action="store_true", help="ì›ë³¸ JSON ì¶œë ¥")

    args = parser.parse_args()

    allowed_domains = [d.strip() for d in args.domains.split(",")] if args.domains else None
    blocked_domains = [d.strip() for d in args.block_domains.split(",")] if args.block_domains else None

    print(f"ğŸ” Claude Web Search: '{args.query}' (mode={args.mode}, model={args.model})", file=sys.stderr)

    result = search(
        args.query,
        mode=args.mode,
        lang=args.lang,
        model=args.model,
        max_search_uses=args.max_searches,
        allowed_domains=allowed_domains,
        blocked_domains=blocked_domains,
        enable_fetch=args.fetch,
        dynamic_filtering=args.dynamic,
    )

    if args.raw:
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        text = extract_response(result)
        print(text)


if __name__ == "__main__":
    main()
