#!/usr/bin/env python3
"""
OpenAI Web Search ìœ í‹¸ë¦¬í‹°
OpenAI Responses APIì˜ web_search ë„êµ¬ë¥¼ í™œìš©í•œ ì›¹ ê²€ìƒ‰ ìŠ¤í¬ë¦½íŠ¸.

ì°¸ê³  ë¬¸ì„œ: https://developers.openai.com/api/docs/guides/tools-web-search/

ì‚¬ìš©ë²•:
    python3 scripts/openai_search.py "ê²€ìƒ‰ì–´"
    python3 scripts/openai_search.py "ê²€ìƒ‰ì–´" --mode search|verify|deep
    python3 scripts/openai_search.py "ê²€ìƒ‰ì–´" --lang ko|en|both
    python3 scripts/openai_search.py "ê²€ìƒ‰ì–´" --domains "pubmed.ncbi.nlm.nih.gov,fda.gov"
"""

import argparse
import json
import os
import sys
from datetime import datetime


def get_api_key():
    key = os.environ.get("OPENAI_API_KEY")
    if not key:
        print("ERROR: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", file=sys.stderr)
        sys.exit(1)
    return key


def search(
    query: str,
    mode: str = "search",
    lang: str = "both",
    model: str = "gpt-4.1",
    allowed_domains: list[str] | None = None,
    user_location: dict | None = None,
) -> dict:
    """
    OpenAI Responses API + web_search ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰.

    Responses APIëŠ” toolsì— {"type": "web_search"}ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
    - ë„ë©”ì¸ í•„í„°: filters.allowed_domains (ìµœëŒ€ 100ê°œ)
    - ìœ„ì¹˜ ê¸°ë°˜: user_location (country, city, region, timezone)
    - ì†ŒìŠ¤ í¬í•¨: include=["web_search_call.action.sources"]
    """
    import urllib.request
    import urllib.error

    api_key = get_api_key()

    # ëª¨ë“œë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompts = {
        "search": (
            "ë‹¹ì‹ ì€ ì›¹ ë¦¬ì„œì¹˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ í¬ê´„ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³ , "
            "í•µì‹¬ ì •ë³´ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ì •ë¦¬í•´ ì£¼ì„¸ìš”. ê° ì •ë³´ì˜ ì¶œì²˜(URL)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”. "
            "í•œêµ­ì–´ì™€ ì˜ì–´ ì†ŒìŠ¤ë¥¼ ëª¨ë‘ í™œìš©í•˜ì„¸ìš”."
        ),
        "verify": (
            "ë‹¹ì‹ ì€ íŒ©íŠ¸ì²´í¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì£¼ì¥/ì •ë³´ì˜ ì •í™•ì„±ì„ ê²€ì¦í•˜ì„¸ìš”. "
            "ì›ë³¸ ì¶œì²˜ë¥¼ ì¶”ì í•˜ê³ , ë‹¤ë¥¸ ì†ŒìŠ¤ì—ì„œì˜ í™•ì¸ ì—¬ë¶€ë¥¼ ë³´ê³ í•˜ì„¸ìš”. "
            "ê²€ì¦ ê²°ê³¼ë¥¼ 'í™•ì¸ë¨/ë¶€ë¶„í™•ì¸/ë¯¸í™•ì¸/ì˜¤ë¥˜'ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”."
        ),
        "deep": (
            "ë‹¹ì‹ ì€ ì‹¬ì¸µ ë¦¬ì„œì¹˜ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ ë‹¤ê°ë„ë¡œ ì‹¬ì¸µ ê²€ìƒ‰í•˜ì„¸ìš”. "
            "ì°¬ì„±/ë°˜ëŒ€ ì–‘ì¸¡ ì˜ê²¬, ì—­ì‚¬ì  ë§¥ë½, ìµœì‹  ë™í–¥, ì „ë¬¸ê°€ ê²¬í•´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”. "
            "í•™ìˆ  ìë£Œ, ì—…ê³„ ë³´ê³ ì„œ, ë‰´ìŠ¤ ê¸°ì‚¬ ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ë¥¼ í™œìš©í•˜ì„¸ìš”."
        ),
    }

    system_prompt = system_prompts.get(mode, system_prompts["search"])

    # ì–¸ì–´ë³„ ê²€ìƒ‰ì–´ ì¡°ì •
    if lang == "ko":
        user_query = f"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ í•œêµ­ì–´ ì†ŒìŠ¤ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê²€ìƒ‰í•´ ì£¼ì„¸ìš”: {query}"
    elif lang == "en":
        user_query = f"Search comprehensively for the following topic using English sources: {query}"
    else:  # both
        user_query = (
            f"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ í•œêµ­ì–´ì™€ ì˜ì–´ ì†ŒìŠ¤ë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ í¬ê´„ì ìœ¼ë¡œ ê²€ìƒ‰í•´ ì£¼ì„¸ìš”: {query}"
        )

    # web_search ë„êµ¬ ì„¤ì •
    web_search_tool = {"type": "web_search"}
    if allowed_domains:
        web_search_tool["filters"] = {"allowed_domains": allowed_domains}
    if user_location:
        web_search_tool["user_location"] = user_location

    payload = {
        "model": model,
        "tools": [web_search_tool],
        "include": ["web_search_call.action.sources"],
        "input": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query},
        ],
    }

    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        "https://api.openai.com/v1/responses",
        data=data,
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}",
        },
        method="POST",
    )

    try:
        with urllib.request.urlopen(req, timeout=120) as resp:
            result = json.loads(resp.read().decode("utf-8"))
    except urllib.error.HTTPError as e:
        error_body = e.read().decode("utf-8") if e.fp else ""
        print(f"ERROR: OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (HTTP {e.code}): {error_body}", file=sys.stderr)
        sys.exit(1)
    except urllib.error.URLError as e:
        print(f"ERROR: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e.reason}", file=sys.stderr)
        sys.exit(1)

    return result


def extract_response(result: dict) -> str:
    """
    Responses API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ì™€ ì¸ìš© ì¶”ì¶œ.

    ì‘ë‹µ êµ¬ì¡° (output ë°°ì—´):
    - type: "web_search_call" â†’ ê²€ìƒ‰ ì‹¤í–‰ ì •ë³´ (status, id)
    - type: "message" â†’ content ë°°ì—´ ë‚´ output_text + annotations
      - annotation.type: "url_citation" â†’ url, title, start_index, end_index
    """
    output_parts = []
    citations = []
    search_sources = []

    if "output" not in result:
        return "(ì‘ë‹µ ì—†ìŒ)"

    for item in result["output"]:
        item_type = item.get("type")

        # ê²€ìƒ‰ í˜¸ì¶œ ì •ë³´
        if item_type == "web_search_call":
            status = item.get("status", "unknown")
            # sourcesê°€ í¬í•¨ëœ ê²½ìš° (include ì˜µì…˜)
            action = item.get("action", {})
            for source in action.get("sources", []):
                search_sources.append({
                    "title": source.get("title", ""),
                    "url": source.get("url", ""),
                })

        # ë©”ì‹œì§€ (í…ìŠ¤íŠ¸ + ì¸ìš©)
        elif item_type == "message":
            for content in item.get("content", []):
                if content.get("type") == "output_text":
                    output_parts.append(content["text"])
                    for annotation in content.get("annotations", []):
                        if annotation.get("type") == "url_citation":
                            citations.append({
                                "title": annotation.get("title", ""),
                                "url": annotation.get("url", ""),
                            })

    text = "\n".join(output_parts)

    # ì¸ìš© URL ì •ë¦¬
    if citations:
        text += "\n\n---\n### ì¸ìš© ì¶œì²˜ (Citations)\n"
        seen = set()
        for c in citations:
            key = c["url"]
            if key and key not in seen:
                seen.add(key)
                text += f"- [{c['title']}]({c['url']})\n"

    # ê²€ìƒ‰ ì†ŒìŠ¤ (include ì˜µì…˜ìœ¼ë¡œ ê°€ì ¸ì˜¨ ì „ì²´ ì†ŒìŠ¤)
    if search_sources:
        text += "\n### ê²€ìƒ‰ ì†ŒìŠ¤ (Sources)\n"
        seen_sources = set()
        for s in search_sources:
            key = s["url"]
            if key and key not in seen_sources:
                seen_sources.add(key)
                text += f"- [{s['title']}]({s['url']})\n"

    return text


def main():
    parser = argparse.ArgumentParser(description="OpenAI Web Search ìœ í‹¸ë¦¬í‹° (Responses API)")
    parser.add_argument("query", help="ê²€ìƒ‰í•  ì£¼ì œ ë˜ëŠ” ì§ˆë¬¸")
    parser.add_argument(
        "--mode",
        choices=["search", "verify", "deep"],
        default="search",
        help="ê²€ìƒ‰ ëª¨ë“œ: search(ì¼ë°˜), verify(íŒ©íŠ¸ì²´í¬), deep(ì‹¬ì¸µ)",
    )
    parser.add_argument(
        "--lang",
        choices=["ko", "en", "both"],
        default="both",
        help="ê²€ìƒ‰ ì–¸ì–´: ko(í•œêµ­ì–´), en(ì˜ì–´), both(ì–‘ìª½)",
    )
    parser.add_argument(
        "--model",
        default="gpt-4.1",
        help="ì‚¬ìš©í•  OpenAI ëª¨ë¸ (ê¸°ë³¸: gpt-4.1)",
    )
    parser.add_argument(
        "--domains",
        default=None,
        help="í—ˆìš© ë„ë©”ì¸ (ì½¤ë§ˆ êµ¬ë¶„, ì˜ˆ: pubmed.ncbi.nlm.nih.gov,fda.gov)",
    )
    parser.add_argument(
        "--country",
        default=None,
        help="ê²€ìƒ‰ ìœ„ì¹˜ êµ­ê°€ ì½”ë“œ (ì˜ˆ: KR, US, GB)",
    )
    parser.add_argument("--raw", action="store_true", help="ì›ë³¸ JSON ì¶œë ¥")

    args = parser.parse_args()

    # ë„ë©”ì¸ í•„í„°
    allowed_domains = None
    if args.domains:
        allowed_domains = [d.strip() for d in args.domains.split(",")]

    # ìœ„ì¹˜ ì„¤ì •
    user_location = None
    if args.country:
        user_location = {"type": "approximate", "country": args.country}

    print(f"ğŸ” OpenAI Web Search: '{args.query}' (mode={args.mode}, lang={args.lang})", file=sys.stderr)

    result = search(
        args.query,
        mode=args.mode,
        lang=args.lang,
        model=args.model,
        allowed_domains=allowed_domains,
        user_location=user_location,
    )

    if args.raw:
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        text = extract_response(result)
        print(text)


if __name__ == "__main__":
    main()
